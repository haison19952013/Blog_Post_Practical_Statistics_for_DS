{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Relationships to Predictions: A Deep Dive into Regression Analysis\n",
    "\n",
    "In data science, understanding how variables relate to one another and making predictions are crucial tasks. This is where regression analysis comes into play, a set of powerful statistical techniques that allows us to model the relationship between variables and make informed predictions. This blog post will delve into the essential concepts of regression, its connection to data science, and how it can be used for predictive modeling.\n",
    "\n",
    "## Main Sections\n",
    "\n",
    "### The Essence of Prediction\n",
    "\n",
    "Regression analysis is fundamentally about creating models that can predict an outcome variable (also called a target variable or dependent variable) using one or more predictor variables (also known as independent variables or features). This process is a type of supervised learning, where the model is trained on data where outcomes are already known. These trained models can then be applied to new data where the outcome is unknown. Beyond prediction, regression diagnostics, which were originally used for improving models, are also useful for anomaly detection.\n",
    "\n",
    "### Core Concepts and Formulas\n",
    "\n",
    "This section will cover some of the basic equations and methods used in regression analysis.\n",
    "\n",
    "#### Simple Linear Regression\n",
    "This models the relationship between a single predictor and an outcome using the equation:\n",
    "\n",
    "$Y = b_0 + b_1*X$\n",
    "\n",
    "Where $Y$ represents the outcome, $X$ is the predictor, $b_0$ is the intercept (the value of Y when $X$ is zero), and $b_1$ is the slope (the change in $Y$ for a one-unit change in $X$).\n",
    "\n",
    "#### Multiple Linear Regression\n",
    "Multiple linear regression extends simple linear regression to incorporate multiple predictors, using the formula:\n",
    "\n",
    "$Y = b_0 + b_1X_1 + b_2X_2 + ... + b_p*X_p$\n",
    "\n",
    "Where $X_1, X_2...X_p$ represent the predictor variables and $b_1, b_2...b_p$ are their corresponding coefficients, quantifying each predictor's impact on the outcome.\n",
    "\n",
    "#### Statistical Measures\n",
    "\n",
    "* Root Mean Squared Error (RMSE): RMSE quantifies the average prediction error and is calculated using the equation:\n",
    "  * RMSE = $\\sqrt{\\sum (y_i − \\hat{y}_i)^2 / n }$\n",
    "  * Where yi represents the actual value, ŷi is the predicted value, and n represents the total number of observations.\n",
    "\n",
    "* Residual Standard Error (RSE): RSE is similar to RMSE, but it adjusts for the number of predictors p in the model:\n",
    "  * RSE = $\\sqrt{\\sum (y_i − \\hat{y}_i)^2 / (n-p-1) }$\n",
    "\n",
    "* Hat-value: Hat-values measure the leverage of each data point, with values above $2*(p+1)/n$ suggesting a high-leverage data point.\n",
    "\n",
    "* Cook's Distance: A metric for identifying influential data points, with values exceeding $4/(n – p – 1)$ indicating a high-influence data point.\n",
    "\n",
    "### Visualizing Relationships\n",
    "\n",
    "Figure 1 illustrates the impact of an influential data point on a regression model. It shows the difference in the regression line when such a point is included or excluded. An influential data point may not be a large outlier, but it can still have high leverage on the regression.\n",
    "\n",
    "| ![influential_data](figure/c4/fig4-5.png) | \n",
    "|:--:| \n",
    "| *Figure 1. An example of an influential data point in regression* |\n",
    "\n",
    "Figure 2 shows an influence plot, which is a combination of standardized residuals, hat values, and Cook's distance. Points with a Cook's distance greater than 0.08 are highlighted to show which data points have the most influence on the model.\n",
    "\n",
    "| ![Cook_distance](figure/c4/fig4-6.png) | \n",
    "|:--:| \n",
    "| *Figure 2. A plot to determine which observations have high influence; points with Cook's distance greater than 0.08 are highlighted in grey* |\n",
    "\n",
    "### Factor Variables\n",
    "\n",
    "Regression models require all input variables to be numeric. If categorical variables are present, they need to be converted into numeric form. One common way to do this is through creating dummy variables, where each category of a factor variable is represented as its own binary (0 or 1) variable. There are various systems for encoding these variables, including deviation and polynomial coding.\n",
    "\n",
    "### Ordered Factor Variables\n",
    "\n",
    "These are categorical variables with an inherent order (e.g., low, medium, high). While factor variables are typically converted to dummy variables, ordered factor variables can often be used directly in a regression model as a numerical variable while preserving the ordering information.\n",
    "\n",
    "### Interactions and Main Effects\n",
    "\n",
    "Regression models can be extended to include interaction effects, which represent how the effect of one predictor can depend on the value of another predictor. For example, the effect of advertising on sales might depend on the season.\n",
    "\n",
    "### Regression Diagnostics\n",
    "\n",
    "This involves analyzing the residuals (the difference between the observed and predicted values). Examining residual plots can help identify non-linearity or heteroscedasticity. Influential points can also be found using influence plots. These plots combine standardized residuals, hat values, and Cook's distance into a single plot.\n",
    "\n",
    "### Nonlinear Relationships\n",
    "\n",
    "When the relationship between the outcome and predictors is non-linear, we can use techniques like polynomial regression, which adds polynomial terms to the regression (e.g., $x^2, x^3$); splines, which are multiple polynomial curves pieced together, or Generalized Additive Models (GAMs), which allow for non-linear relationships.\n",
    "\n",
    "### Extrapolation\n",
    "\n",
    "It is risky to extrapolate beyond the range of your data, as this may lead to large errors.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "* Model Evaluation Metrics:\n",
    "  * RMSE and RSE are used to measure the overall prediction error\n",
    "  * R-squared quantifies the proportion of variance in the outcome explained by the model\n",
    "  * Adjusted R-squared accounts for the number of predictors, penalizing overly complex models\n",
    "  * AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) are used to evaluate models with penalties for adding extra variables\n",
    "\n",
    "* Variable Selection: Stepwise selection is an iterative process to determine the most significant predictors by adding or removing variables\n",
    "\n",
    "* Factor Variables: These must be converted into numeric variables for use in regression, using methods like one-hot encoding or dummy variables\n",
    "\n",
    "* Ordered Factors: When factor variables have an inherent order, this information should be used in the regression model\n",
    "\n",
    "* Interactions: They allow the model to capture situations where two or more variables have a combined effect on the outcome\n",
    "\n",
    "* Non-Linearity: Non-linear relationships can be modeled using techniques like polynomial regression, splines, or GAMs\n",
    "\n",
    "* Influential Observations: Outliers and influential points can skew model results and should be examined carefully\n",
    "\n",
    "* Extrapolation: Predictions should not be made outside the range of the data used to fit the model\n",
    "\n",
    "* Weighted Regression: This allows you to assign different levels of importance to different records in the regression model\n",
    "\n",
    "\n",
    "## In Conclusion\n",
    "\n",
    "Regression analysis is essential for data scientists, providing a way to model relationships and predict outcomes. By mastering these concepts, you can effectively analyze data, build predictive models, and gain valuable insights. Remember to always explore your data, validate your model, and carefully interpret your results. The open-source community has contributed a great deal in this area and has developed many tools that, combined with the expressiveness of R and Python, help to explore and analyze data in a variety of ways.\n",
    "\n",
    "## Appendix: Algorithms Mentioned in Chapter 4\n",
    "### Creating Dummy Variables: \n",
    "When incorporating categorical (factor) variables into a regression model, they must be converted into numeric variables. One common approach is to use dummy variables.\n",
    "1. For a categorical variable with P distinct values, create P - 1 new variables.\n",
    "2. Each of these new variables will represent a single category, and will be assigned a value of 1 if a record belongs to that category and 0 otherwise.\n",
    "\n",
    "### Stepwise Selection: This family of algorithms iteratively adds or removes predictors from a regression model based on statistical criteria.\n",
    "- Forward Selection:\n",
    "1. Start with a model with no predictors.\n",
    "2. Iteratively add the predictor that results in the greatest improvement in model fit.\n",
    "3. Stop when adding more predictors does not significantly improve the model fit.\n",
    "\n",
    "- Backward Elimination:\n",
    "1. Start with a model that includes all predictors.\n",
    "2. Iteratively remove the predictor that contributes the least to the model fit.\n",
    "3. Stop when removing predictors no longer improves the model fit.\n",
    "\n",
    "### Bootstrap Algorithm for Confidence Intervals for Regression Coefficients:\n",
    "1. Create a bootstrap sample: Consider each row (including the outcome variable) as a single \"ticket.\" Place all n tickets in a box.\n",
    "2. Draw a ticket at random, record the values, and replace it in the box.\n",
    "3. Repeat step 2 n times to create one bootstrap resample.\n",
    "4. Fit a regression to the bootstrap sample, and record the estimated coefficients.\n",
    "5. Repeat steps 1-4 many times (e.g., 1,000 times).\n",
    "6. Calculate the confidence interval: Find the appropriate percentiles for each coefficient (e.g., the 5th and 95th for a 90% confidence interval) based on the 1,000 bootstrap values.\n",
    "\n",
    "### Bootstrap Algorithm for Prediction Intervals:\n",
    "This algorithm incorporates both the uncertainty in the regression model and the individual data point error.\n",
    "1. Create a bootstrap sample: Take a bootstrap sample from the original data (using the same procedure as in the confidence interval algorithm).\n",
    "2. Fit the regression and predict a new value: Fit the regression to the bootstrap sample and generate a predicted value.\n",
    "3. Add a residual: Take a single residual at random from the original regression fit. Add it to the predicted value and record the result.\n",
    "4. Repeat Steps 1-3 many times (e.g., 1,000 times).\n",
    "5. Calculate the prediction interval: Find the desired percentiles of the results (e.g., the 2.5th and 97.5th percentiles for a 95% prediction interval).\n",
    "\n",
    "### Local Regression (LOESS): \n",
    "This method is used to create smoothers for visualizing relationships between residuals and predicted values in regression. The algorithm is as follows:\n",
    "1. Define local neighborhoods: For any given value of x, define a \"local neighborhood\" of points around that x-value.\n",
    "2. Fit a simple regression within the neighborhood: Fit a regression model that gives more weight to the points close to x.\n",
    "3. Calculate the fitted value at x: Use the fitted regression in the neighborhood to predict a value at x.\n",
    "4. Repeat for all x: Repeat for all x-values in the range, generating a line that reflects the average relationship in the data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
