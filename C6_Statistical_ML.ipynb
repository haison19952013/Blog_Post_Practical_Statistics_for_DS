{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Machine Learning: Unleashing the Power of Predictive Models\n",
    "\n",
    "Chapter 6 of \"Practical Statistics for Data Scientists\" dives into the exciting realm of statistical machine learning, highlighting techniques that provide automated and powerful approaches for both regression and classification tasks. These supervised methods learn from data where the outcomes are known, but they stand out due to their data-driven nature, which avoids imposing linear or other structural assumptions.\n",
    "\n",
    "## K-Nearest Neighbors: Learning from Your Data's Neighborhood\n",
    "\n",
    "The chapter begins with K-Nearest Neighbors (KNN), a straightforward algorithm that classifies data points based on the classification of similar nearby points.\n",
    "\n",
    "* **How it Works**: KNN uses distance metrics to determine the 'K' closest neighbors to a data point. For classification, the new point is assigned to the majority class among its neighbors. For regression, the new point is assigned the average of the target variable values of its K nearest neighbors.\n",
    "\n",
    "* **Flexibility**: KNN can output a probability (or propensity) between 0 and 1, which is based on the proportion of one class among the K nearest neighbors.\n",
    "\n",
    "* **Data Preprocessing**: KNN often requires one-hot encoding for categorical variables, and standardization (also called normalization) of numerical variables. Standardization puts all variables on a similar scale by subtracting the mean and dividing by the standard deviation. This ensures that a variable does not overly influence the model simply due to the scale of its original measurement.\n",
    "\n",
    "### Standardization Formula:\n",
    "```\n",
    "z = (x - μ) / σ\n",
    "```\n",
    "where x is the original value, μ is the mean and σ is the standard deviation.\n",
    "\n",
    "*Figure 1 illustrates KNN prediction of loan default using two variables: debt-to-income ratio and loan-payment-to-income ratio.*\n",
    "\n",
    "| ![KNN](figure/c6/fig6-2.png) | \n",
    "|:--:| \n",
    "| *Figure 1.  KNN prediction of loan default using two variables: debt-to-income ratio and loan-payment-to-income ratio* |\n",
    "\n",
    "## Tree Models: Decision-Making Through Rules\n",
    "\n",
    "Next, the chapter explores tree models, which classify or predict outcomes using a series of \"if-then-else\" rules.\n",
    "\n",
    "* **Recursive Partitioning**: Tree models use a recursive partitioning algorithm to repeatedly divide the data into increasingly homogeneous partitions based on the values of predictor variables.\n",
    "\n",
    "* **Interpretability**: Tree models have the advantage of being interpretable, clearly showing the rules that contribute to the final classification or prediction.\n",
    "\n",
    "* **Impurity**: The algorithm selects the best splits by measuring the \"impurity\" of the resulting groups. Two common measures of impurity are Gini impurity and entropy of information.\n",
    "\n",
    "### Impurity Formulas:\n",
    "```\n",
    "Gini Impurity: IA = p(1 - p)\n",
    "```\n",
    "where 'p' is the proportion of one class in a set of records 'A'.\n",
    "\n",
    "```\n",
    "Entropy: IA = -p * log2(p) - (1 - p) * log2(1 - p)\n",
    "```\n",
    "\n",
    "*Figure 2 displays the similarity of Gini impurity and entropy measures.*\n",
    "\n",
    "| ![KNN](figure/c6/fig6-5.png) | \n",
    "|:--:| \n",
    "| *Figure 2.  Gini impurity and entropy measures* |\n",
    "\n",
    "* **Tree Pruning**: To avoid overfitting, trees are often pruned to determine the optimal tree complexity. Pruning is often accomplished by using cross-validation to select the optimal complexity parameter (cp) which corresponds to the minimum error on validation data.\n",
    "\n",
    "## Ensemble Methods: Combining the Wisdom of Many Models\n",
    "\n",
    "The chapter then introduces ensemble methods, which combine multiple models to form a prediction.\n",
    "\n",
    "* **Bagging**: Also known as bootstrap aggregating, bagging fits multiple models to different bootstrap resamples of the data and averages their predictions. The general idea of bagging is to use many models to form a prediction, as opposed to using just a single model.\n",
    "\n",
    "* **Random Forest**: A variant of bagging, random forests apply bagging to decision trees but also include the additional step of randomly sampling predictor variables at each split. At each stage of the algorithm, the choice of variable is limited to a random subset of variables. This adds more diversity and robustness to the model.\n",
    "\n",
    "Variable Importance can be measured by:\n",
    "- Mean decrease in accuracy\n",
    "- Mean decrease in the Gini impurity\n",
    "\n",
    "*Figure 3 shows variable importance according to decrease in accuracy and Gini impurity.*\n",
    "| ![variable_importance](figure/c6/fig6-8.png) | \n",
    "|:--:| \n",
    "| *Figure 3. The importance of variables for the full model fit to the loan data* |\n",
    "\n",
    "\n",
    "* **Boosting**: Boosting fits a sequence of models with each successive model trying to minimize the errors of the previous model. Gradient boosting and stochastic gradient boosting are popular versions. XGBoost, an implementation of stochastic gradient boosting, is widely used and is computationally efficient.\n",
    "\n",
    "## Key Takeaways from the Chapter\n",
    "\n",
    "* Statistical machine learning provides powerful automated techniques for predictive modeling.\n",
    "* KNN classifies data based on the classification of their neighbors.\n",
    "* Tree models use a series of if-then-else rules to make predictions and are highly interpretable.\n",
    "* Ensemble methods, like bagging, random forests, and boosting, improve model accuracy by combining multiple models.\n",
    "* Random forests and boosted trees often outperform single-tree models in predictive accuracy, but sacrifice the simple interpretability of single trees.\n",
    "\n",
    "*Figure 4 displays the predicted outcomes from the random forest algorithm applied to loan default data.*\n",
    "\n",
    "| ![random_forest](figure/c6/fig6-7.png) | \n",
    "|:--:| \n",
    "| *Figure 4. The predicted outcomes from the random forest applied to the loan default data* |\n",
    "\n",
    "## Appendix: Algorithm Summaries\n",
    "\n",
    "### K-Nearest Neighbors (KNN)\n",
    "\n",
    "* **Goal**: To classify or predict a record based on the majority class (or average value) of its 'K' nearest neighbors.\n",
    "* **Method**: Calculates distances between a record and all other records, selects the k nearest neighbors, and assigns the class based on majority vote or average.\n",
    "* **Key Steps**:\n",
    "   1. Choose the value of 'K'\n",
    "   2. Calculate the distance to all data points using a distance metric such as Euclidean distance\n",
    "   3. Select the k nearest neighbors\n",
    "   4. Classify the data point based on the most frequent class (or average value) of the k nearest neighbors\n",
    "   5. Determine K based on model performance\n",
    "* **Data**: Requires scaled numerical predictors and one hot encoded categorical predictors\n",
    "\n",
    "### Recursive Partitioning Algorithm for Tree Models\n",
    "\n",
    "* **Goal**: To create a tree-like structure to classify or predict data by recursively splitting the data into more homogeneous subgroups.\n",
    "* **Method**: Finds the best split at each step by selecting a variable and split point that maximizes homogeneity in the resulting subgroups, using metrics like Gini impurity or entropy.\n",
    "* **Key Steps**:\n",
    "   1. Start with the entire dataset as a single group\n",
    "   2. For each variable, find the optimal split point that best separates the data into more homogenous subgroups\n",
    "   3. Choose the variable and split point that results in the greatest homogeneity\n",
    "   4. Recursively apply this process on the newly created subgroups until the tree is fully grown\n",
    "* **Output**: \"If-then-else\" rules leading to a classification or prediction\n",
    "\n",
    "### Bagging (Bootstrap Aggregating)\n",
    "\n",
    "* **Goal**: To reduce variance and improve prediction accuracy by fitting multiple models to bootstrap resamples of the data and averaging their predictions.\n",
    "* **Method**: Creates multiple bootstrap samples from the original data and fits a model to each sample.\n",
    "* **Key Steps**:\n",
    "   1. Create multiple bootstrap samples from the original data\n",
    "   2. Fit a model to each bootstrap sample\n",
    "   3. Average the predictions of all the models to form the final prediction\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "* **Goal**: Extend bagging to decision trees by sampling variables to create more diverse trees which can reduce variance and improve prediction accuracy.\n",
    "* **Method**: Applies bagging to decision trees, but also randomly samples a subset of predictor variables at each split.\n",
    "* **Key Steps**:\n",
    "   1. Create multiple bootstrap samples from the original data\n",
    "   2. For each bootstrap sample, grow a tree. At each split, consider only a random subset of predictor variables\n",
    "   3. Average the predictions of all the trees\n",
    "\n",
    "### Boosting (General Algorithm)\n",
    "\n",
    "* **Goal**: To improve prediction accuracy by iteratively building models that focus on the errors of the previous model.\n",
    "* **Method**: Fit a series of models, with each successive model focusing on the records that had large errors in the previous model.\n",
    "* **Key Steps**:\n",
    "   1. Fit an initial model to the data\n",
    "   2. Compute the residuals\n",
    "   3. Fit the next model to the residuals\n",
    "   4. Combine the models in a weighted manner to form the final model\n",
    "   5. Several variants include Adaboost, gradient boosting and stochastic gradient boosting"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
